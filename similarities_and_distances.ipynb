{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Similarities & Distances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Поиск ключевых слов по 4-м тематикам\n",
    "- спорт\n",
    "- наука\n",
    "- шоппинг\n",
    "- новости\n",
    "\n",
    "На просторах интернета есть куча сайтов с базами данных ключевых слов по определённой тематике, но, of course, почти все они платные.... :sob:\n",
    "\n",
    "Сначала я хотела составить базу слов, основываясь на данных с сервисов [Google Trends](https://trends.google.ru/trends/) (у этого сервиса удобнее интерфейс, и есть возможность скачать .csv с related запросами) и [Yandex Wordstat](https://wordstat.yandex.ru/). Но они предоставляют ограниченное кол-во слов, и в большинстве случаев предоставляемые слова (по моему мнению) не очень хорошо описывают данную тематику.\n",
    "\n",
    "Поэтому я использовала сервис [kartaslov.ru](https://kartaslov.ru/), на этом сайте можно получить синонимичные слова, слова, которые ассоциируются с данным топиком - это является более характеризующим/описательным для определения принадлежности какого-то текста к данной тематике. (Найденные слова также подверглись манипуляциям из 1 пункта, удалены дубликаты)\n",
    "\n",
    "Слова находятся в директории data/topics.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Prepocessing\n",
    "\n",
    "Предобработаем выбранный текст, находящийся в файле data/oreologia.txt.\n",
    "Текст взят с этого занимательного поста на хабре - [ссылка](https://habr.com/ru/company/ua-hosting/blog/663380/)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/renett/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/renett/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Токенизируем (разбиваем текст на единицы - слова, знаки препинания)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3577\n"
     ]
    }
   ],
   "source": [
    "file_name = \"data/oreologia.txt\"\n",
    "\n",
    "with open(file_name, 'r') as reader:\n",
    "    text = reader.readlines()\n",
    "\n",
    "tokens = []\n",
    "for line in text:\n",
    "    tokens += word_tokenize(line)\n",
    "\n",
    "# print(tokens)\n",
    "print(len(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Убираем стоп-слова и символы пунктуации:\n",
    "\n",
    "(тут, по-хорошему, нужно было бы ещё удалить и символы латинского алфавита - они использовались для описания формул в данном тексте)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', 'т.', 'д', '—', '«', '»']\n"
     ]
    }
   ],
   "source": [
    "# импортируем стоп-слова из библиотеки nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "noise = stopwords.words('russian') + list(punctuation)\n",
    "noise.append(\"т.\")\n",
    "noise.append(\"д\")\n",
    "noise.append(\"—\")\n",
    "noise.append(\"«\")\n",
    "noise.append(\"»\")\n",
    "\n",
    "print(noise)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after preprocessing: 2186\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = [x for x in tokens if x.lower() not in noise]\n",
    "\n",
    "print('Tokens after preprocessing:', len(filtered_tokens))\n",
    "# print(filtered_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "А теперь получим начальные формы слов - лемматизируем, используя морфологический анализатор MorphAnalyzer из pymorphy2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "pymorphy2_analyzer = MorphAnalyzer()\n",
    "\n",
    "final_tokens = []\n",
    "for token in filtered_tokens:\n",
    "    lem = pymorphy2_analyzer.parse(token)\n",
    "    final_tokens.append(lem[0].normal_form)\n",
    "# print(final_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Считываем слова из топиков: news_tokens, science_tokens, shopping_tokens and sport_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data/topics/news.txt\", 'r') as reader:\n",
    "    text = reader.readlines()\n",
    "\n",
    "news_tokens = []\n",
    "for line in text:\n",
    "    news_tokens += word_tokenize(line)\n",
    "\n",
    "with open(\"data/topics/science.txt\", 'r') as reader:\n",
    "    text = reader.readlines()\n",
    "\n",
    "science_tokens = []\n",
    "for line in text:\n",
    "    science_tokens += word_tokenize(line)\n",
    "\n",
    "with open(\"data/topics/shopping.txt\", 'r') as reader:\n",
    "    text = reader.readlines()\n",
    "\n",
    "shopping_tokens = []\n",
    "for line in text:\n",
    "    shopping_tokens += word_tokenize(line)\n",
    "\n",
    "with open(\"data/topics/sport.txt\", 'r') as reader:\n",
    "    text = reader.readlines()\n",
    "\n",
    "sport_tokens = []\n",
    "for line in text:\n",
    "    sport_tokens += word_tokenize(line)\n",
    "\n",
    "topics_titles = 'news', 'science', 'shopping', 'sport'\n",
    "topics_tokens = news_tokens, science_tokens, shopping_tokens, sport_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 3. Выссчитываем схожесть текста к топикам по метрике Жаккарда.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def jaccard_similarity(one, another):\n",
    "    intersection = len(set.intersection(*[set(one), set(another)]))\n",
    "    union = len(set.union(*[set(one), set(another)]))\n",
    "    return intersection / float(union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity with topic \"news\" = 0.023\n",
      "Jaccard similarity with topic \"science\" = 0.064\n",
      "Jaccard similarity with topic \"shopping\" = 0.015\n",
      "Jaccard similarity with topic \"sport\" = 0.029\n"
     ]
    }
   ],
   "source": [
    "jaccard_similarities = []\n",
    "\n",
    "for topic in topics_tokens:\n",
    "    jaccard_similarities.append(jaccard_similarity(final_tokens, topic))\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print('Jaccard similarity with topic \"%s\" = %.3f' % (topics_titles[i], jaccard_similarities[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 4. Выссчитываем схожесть текста к топикам по метрике Косинуса.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy\n",
    "\n",
    "\n",
    "def squared_sum(x):\n",
    "    return round(sqrt(sum([a * a for a in x])), 5)\n",
    "\n",
    "\n",
    "def count_occurrences(elem, elems):\n",
    "    count = 0\n",
    "    elems = list(elems)\n",
    "    for k in range(0, len(elems)):\n",
    "        if elem == elems[k]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def cosine_similarity(one, another):\n",
    "    all_elems = set(list(one) + list(another))\n",
    "\n",
    "    x = numpy.zeros(len(all_elems), dtype=int)\n",
    "    y = numpy.zeros(len(all_elems), dtype=int)\n",
    "\n",
    "    for i in range(0, len(all_elems)):\n",
    "        elems_list = list(all_elems)\n",
    "        x[i] = count_occurrences(elems_list[i], one)\n",
    "        y[i] = count_occurrences(elems_list[i], another)\n",
    "\n",
    "    numerator = sum(a * b for a, b in zip(x, y))\n",
    "    denominator = squared_sum(x) * squared_sum(y)\n",
    "    return round(numerator / float(denominator), 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity with topic \"news\" = 0.029\n",
      "Cosine similarity with topic \"science\" = 0.088\n",
      "Cosine similarity with topic \"shopping\" = 0.021\n",
      "Cosine similarity with topic \"sport\" = 0.040\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = []\n",
    "\n",
    "for topic in topics_tokens:\n",
    "    cosine_similarities.append(cosine_similarity(final_tokens, topic))\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print('Cosine similarity with topic \"%s\" = %.3f' % (topics_titles[i], cosine_similarities[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "## 5. Результат:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is from file data/oreologia.txt \n",
      "\n",
      "Jaccard similarity with topic \"news\" = 0.023\n",
      "Jaccard similarity with topic \"science\" = 0.064\n",
      "Jaccard similarity with topic \"shopping\" = 0.015\n",
      "Jaccard similarity with topic \"sport\" = 0.029\n",
      "\n",
      "\n",
      "Cosine similarity with topic \"news\" = 0.029\n",
      "Cosine similarity with topic \"science\" = 0.088\n",
      "Cosine similarity with topic \"shopping\" = 0.021\n",
      "Cosine similarity with topic \"sport\" = 0.040\n"
     ]
    }
   ],
   "source": [
    "print('Text is from file', file_name, '\\n')\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print('Jaccard similarity with topic \"%s\" = %.3f' % (topics_titles[i], jaccard_similarities[i]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print('Cosine similarity with topic \"%s\" = %.3f' % (topics_titles[i], cosine_similarities[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------\n",
    "\n",
    "\n",
    "Видим, что по обеим метрикам текст с [хабра про исследование печенья Орео](https://habr.com/ru/company/ua-hosting/blog/663380/) ближе по содержанию к теме \"НАУКА\" (результат совпал с ожидаемым).\n",
    "\n",
    "\n",
    "## Рейтинг тем, к которым ближе всего данный текст, по метрике косинуса:\n",
    "1. наука - 0.064\n",
    "2. спорт - 0.029\n",
    "3. новости - 0.023\n",
    "4. шоппинг - 0.015\n",
    "\n",
    "## Рейтинг тем, к которым ближе всего данный текст, по метрике косинуса:\n",
    "1. наука - 0.088\n",
    "2. спорт - 0.040\n",
    "3. новости - 0.020\n",
    "4. шоппинг - 0.021\n",
    "\n",
    "Рейтинги совпадают.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}